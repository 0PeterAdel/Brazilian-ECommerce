{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2669146,"sourceType":"datasetVersion","datasetId":55151}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# For Full Project >> github.com/0PeterAdel/Brazilian-ECommerce\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:30:56.581580Z","iopub.execute_input":"2025-02-28T14:30:56.582004Z","iopub.status.idle":"2025-02-28T14:30:57.043340Z","shell.execute_reply.started":"2025-02-28T14:30:56.581973Z","shell.execute_reply":"2025-02-28T14:30:57.042224Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/brazilian-ecommerce/olist_customers_dataset.csv\n/kaggle/input/brazilian-ecommerce/olist_sellers_dataset.csv\n/kaggle/input/brazilian-ecommerce/olist_order_reviews_dataset.csv\n/kaggle/input/brazilian-ecommerce/olist_order_items_dataset.csv\n/kaggle/input/brazilian-ecommerce/olist_products_dataset.csv\n/kaggle/input/brazilian-ecommerce/olist_geolocation_dataset.csv\n/kaggle/input/brazilian-ecommerce/product_category_name_translation.csv\n/kaggle/input/brazilian-ecommerce/olist_orders_dataset.csv\n/kaggle/input/brazilian-ecommerce/olist_order_payments_dataset.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"**Load the All DataSet**","metadata":{}},{"cell_type":"code","source":"root_path = \"/kaggle/input/brazilian-ecommerce/\"\noutput_path = \"/kaggle/working/\"\n\ncustomers = pd.read_csv(root_path + 'olist_customers_dataset.csv')\nsellers = pd.read_csv(root_path + 'olist_sellers_dataset.csv')\norder_reviews = pd.read_csv(root_path + 'olist_order_reviews_dataset.csv')\norder_items = pd.read_csv(root_path + 'olist_order_items_dataset.csv')\nproducts = pd.read_csv(root_path + 'olist_products_dataset.csv')\ngeolocation = pd.read_csv(root_path + 'olist_geolocation_dataset.csv')\ncategories = pd.read_csv(root_path + 'product_category_name_translation.csv')\norders = pd.read_csv(root_path + 'olist_orders_dataset.csv')\norder_payments = pd.read_csv(root_path + 'olist_order_payments_dataset.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:41:35.905834Z","iopub.execute_input":"2025-02-28T14:41:35.906286Z","iopub.status.idle":"2025-02-28T14:41:38.611819Z","shell.execute_reply.started":"2025-02-28T14:41:35.906255Z","shell.execute_reply":"2025-02-28T14:41:38.610563Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"---\n---\n\n## 1.  olist_customers_dataset","metadata":{}},{"cell_type":"code","source":"# 1. Inspect the data\n\nprint(\"Initial Customers Dataset Info:\")\nprint(customers.info())\nprint(\"\\nMissing Values:\")\nprint(customers.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:44:24.292579Z","iopub.execute_input":"2025-02-28T14:44:24.292957Z","iopub.status.idle":"2025-02-28T14:44:24.360288Z","shell.execute_reply.started":"2025-02-28T14:44:24.292929Z","shell.execute_reply":"2025-02-28T14:44:24.359091Z"}},"outputs":[{"name":"stdout","text":"Initial Customers Dataset Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 99441 entries, 0 to 99440\nData columns (total 6 columns):\n #   Column                    Non-Null Count  Dtype \n---  ------                    --------------  ----- \n 0   customer_id               99441 non-null  object\n 1   customer_unique_id        99441 non-null  object\n 2   customer_zip_code_prefix  99441 non-null  object\n 3   customer_city             99441 non-null  object\n 4   customer_state            99441 non-null  object\n 5   is_valid_state            99441 non-null  int64 \ndtypes: int64(1), object(5)\nmemory usage: 4.6+ MB\nNone\n\nMissing Values:\ncustomer_id                 0\ncustomer_unique_id          0\ncustomer_zip_code_prefix    0\ncustomer_city               0\ncustomer_state              0\nis_valid_state              0\ndtype: int64\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# 2. Drop rows with missing critical IDs\ncustomers = customers.dropna(subset=['customer_id', 'customer_unique_id'])\n\n# 3. Fill missing geographic data with 'Unknown'\ncustomers['customer_zip_code_prefix'] = customers['customer_zip_code_prefix'].fillna('Unknown')\ncustomers['customer_city'] = customers['customer_city'].fillna('Unknown')\ncustomers['customer_state'] = customers['customer_state'].fillna('Unknown')\n\n# 4. Convert to consistent data types\ncustomers['customer_id'] = customers['customer_id'].astype(str)\ncustomers['customer_unique_id'] = customers['customer_unique_id'].astype(str)\ncustomers['customer_zip_code_prefix'] = customers['customer_zip_code_prefix'].astype(str)\n\n# 5. Standardize text\ncustomers['customer_city'] = customers['customer_city'].str.strip().str.title()\ncustomers['customer_state'] = customers['customer_state'].str.strip().str.upper()\n\n# 6. Remove duplicate customer_ids\ncustomers = customers.drop_duplicates(subset=['customer_id'], keep='first')\n\n# 7. Creative addition: Validate Brazilian state codes\nvalid_states = {'AC', 'AL', 'AP', 'AM', 'BA', 'CE', 'DF', 'ES', 'GO', 'MA', 'MT', \n                'MS', 'MG', 'PA', 'PB', 'PR', 'PE', 'PI', 'RJ', 'RN', 'RS', 'RO', \n                'RR', 'SC', 'SP', 'SE', 'TO'}\ncustomers['is_valid_state'] = customers['customer_state'].isin(valid_states).astype(int)\n\n# 8. Save the cleaned file\ncustomers.to_csv(output_path + 'cleaned_olist_customers_dataset.csv', index=False)\nprint(\"Saved cleaned customers dataset as '/Data_Cleaned/cleaned_olist_customers_dataset.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:42:33.674080Z","iopub.execute_input":"2025-02-28T14:42:33.674608Z","iopub.status.idle":"2025-02-28T14:42:34.258167Z","shell.execute_reply.started":"2025-02-28T14:42:33.674573Z","shell.execute_reply":"2025-02-28T14:42:34.256880Z"}},"outputs":[{"name":"stdout","text":"Saved cleaned customers dataset as '/Data_Cleaned/cleaned_olist_customers_dataset.csv'\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"---\n---\n## 2.  olist_sellers_dataset","metadata":{}},{"cell_type":"code","source":"# 1. Inspect the data\n\nprint(\"Initial Sellers Dataset Info:\")\nprint(sellers.info())\nprint(\"\\nMissing Values:\")\nprint(sellers.isnull().sum())\nprint(\"\\nDuplicate Seller IDs:\")\nprint(sellers['seller_id'].duplicated().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:45:04.339769Z","iopub.execute_input":"2025-02-28T14:45:04.340279Z","iopub.status.idle":"2025-02-28T14:45:04.362944Z","shell.execute_reply.started":"2025-02-28T14:45:04.340241Z","shell.execute_reply":"2025-02-28T14:45:04.361517Z"}},"outputs":[{"name":"stdout","text":"Initial Sellers Dataset Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3095 entries, 0 to 3094\nData columns (total 4 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   seller_id               3095 non-null   object\n 1   seller_zip_code_prefix  3095 non-null   int64 \n 2   seller_city             3095 non-null   object\n 3   seller_state            3095 non-null   object\ndtypes: int64(1), object(3)\nmemory usage: 96.8+ KB\nNone\n\nMissing Values:\nseller_id                 0\nseller_zip_code_prefix    0\nseller_city               0\nseller_state              0\ndtype: int64\n\nDuplicate Seller IDs:\n0\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# 2. Handle missing values\n# Check for missing values in critical columns\nif sellers['seller_id'].isnull().sum() > 0:\n    sellers = sellers.dropna(subset=['seller_id'])  # Drop rows with missing seller_id\n    print(\"Dropped rows with missing seller_id\")\n\n# For zip code, city, and state, fill missing values with 'Unknown' to preserve data\nsellers['seller_zip_code_prefix'] = sellers['seller_zip_code_prefix'].fillna('Unknown')\nsellers['seller_city'] = sellers['seller_city'].fillna('Unknown')\nsellers['seller_state'] = sellers['seller_state'].fillna('Unknown')\n\n# 3. Ensure data type consistency\n# seller_id should be string\nsellers['seller_id'] = sellers['seller_id'].astype(str)\n# seller_zip_code_prefix might be numeric or string; convert to string for consistency\nsellers['seller_zip_code_prefix'] = sellers['seller_zip_code_prefix'].astype(str)\n# seller_city and seller_state should be strings\nsellers['seller_city'] = sellers['seller_city'].astype(str)\nsellers['seller_state'] = sellers['seller_state'].astype(str)\n\n# 4. Standardize text data\n# Remove leading/trailing whitespace and convert cities/states to title case\nsellers['seller_city'] = sellers['seller_city'].str.strip().str.title()\nsellers['seller_state'] = sellers['seller_state'].str.strip().str.upper()\n\n# 5. Check for duplicates\n# Remove duplicate seller_id entries, keeping the first occurrence\nsellers = sellers.drop_duplicates(subset=['seller_id'], keep='first')\nprint(f\"Removed {sellers['seller_id'].duplicated().sum()} duplicate seller IDs\")\n\n# 6. Validate state codes (Creative Touch)\n# Brazilian states have 2-letter codes; flag any anomalies\nvalid_states = {'AC', 'AL', 'AP', 'AM', 'BA', 'CE', 'DF', 'ES', 'GO', 'MA', 'MT', \n                'MS', 'MG', 'PA', 'PB', 'PR', 'PE', 'PI', 'RJ', 'RN', 'RS', 'RO', \n                'RR', 'SC', 'SP', 'SE', 'TO'}\nsellers['is_valid_state'] = sellers['seller_state'].isin(valid_states).astype(int)\ninvalid_states = sellers[~sellers['seller_state'].isin(valid_states)]['seller_state'].unique()\nif len(invalid_states) > 0:\n    print(f\"Invalid state codes detected: {invalid_states}\")\n\n# 7. Creative Addition - Derive region from state\n# Mapping Brazilian states to regions for geographic analysis\nstate_to_region = {\n    'AC': 'North', 'AM': 'North', 'AP': 'North', 'PA': 'North', 'RO': 'North', 'RR': 'North', 'TO': 'North',\n    'AL': 'Northeast', 'BA': 'Northeast', 'CE': 'Northeast', 'MA': 'Northeast', 'PB': 'Northeast', \n    'PE': 'Northeast', 'PI': 'Northeast', 'RN': 'Northeast', 'SE': 'Northeast',\n    'DF': 'Central-West', 'GO': 'Central-West', 'MT': 'Central-West', 'MS': 'Central-West',\n    'ES': 'Southeast', 'MG': 'Southeast', 'RJ': 'Southeast', 'SP': 'Southeast',\n    'PR': 'South', 'RS': 'South', 'SC': 'South'\n}\nsellers['seller_region'] = sellers['seller_state'].map(state_to_region).fillna('Unknown')\nprint(\"Added seller_region column for geographic insights\")\n\n\n# 8. Save the cleaned dataset\nsellers.to_csv(output_path + 'cleaned_sellers_dataset.csv', index=False)\nprint(\"Sellers dataset cleaned and saved as 'cleaned_sellers_dataset.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:48:36.564194Z","iopub.execute_input":"2025-02-28T14:48:36.564566Z","iopub.status.idle":"2025-02-28T14:48:36.606488Z","shell.execute_reply.started":"2025-02-28T14:48:36.564541Z","shell.execute_reply":"2025-02-28T14:48:36.605312Z"}},"outputs":[{"name":"stdout","text":"Removed 0 duplicate seller IDs\nAdded seller_region column for geographic insights\nSellers dataset cleaned and saved as 'cleaned_sellers_dataset.csv'\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"---\n---\n## 3. olist_order_reviews_dataset","metadata":{}},{"cell_type":"code","source":"# 1. Inspect the data\n\nprint(\"Initial Order Reviews Dataset Info:\")\nprint(order_reviews.info())\nprint(\"\\nMissing Values:\")\nprint(order_reviews.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:51:34.669225Z","iopub.execute_input":"2025-02-28T14:51:34.669606Z","iopub.status.idle":"2025-02-28T14:51:34.742565Z","shell.execute_reply.started":"2025-02-28T14:51:34.669577Z","shell.execute_reply":"2025-02-28T14:51:34.741447Z"}},"outputs":[{"name":"stdout","text":"Initial Order Reviews Dataset Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 99224 entries, 0 to 99223\nData columns (total 7 columns):\n #   Column                   Non-Null Count  Dtype \n---  ------                   --------------  ----- \n 0   review_id                99224 non-null  object\n 1   order_id                 99224 non-null  object\n 2   review_score             99224 non-null  int64 \n 3   review_comment_title     11568 non-null  object\n 4   review_comment_message   40977 non-null  object\n 5   review_creation_date     99224 non-null  object\n 6   review_answer_timestamp  99224 non-null  object\ndtypes: int64(1), object(6)\nmemory usage: 5.3+ MB\nNone\n\nMissing Values:\nreview_id                      0\norder_id                       0\nreview_score                   0\nreview_comment_title       87656\nreview_comment_message     58247\nreview_creation_date           0\nreview_answer_timestamp        0\ndtype: int64\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# 2. Drop rows with missing critical IDs\norder_reviews = order_reviews.dropna(subset=['review_id', 'order_id'])\n\n# 3. Fill missing comments\norder_reviews['review_comment_title'] = order_reviews['review_comment_title'].fillna('No Comment')\norder_reviews['review_comment_message'] = order_reviews['review_comment_message'].fillna('No Comment')\n\n# 4. Convert data types\norder_reviews['review_id'] = order_reviews['review_id'].astype(str)\norder_reviews['order_id'] = order_reviews['order_id'].astype(str)\norder_reviews['review_score'] = order_reviews['review_score'].astype(int)\norder_reviews['review_creation_date'] = pd.to_datetime(order_reviews['review_creation_date'], errors='coerce')\norder_reviews['review_answer_timestamp'] = pd.to_datetime(order_reviews['review_answer_timestamp'], errors='coerce')\n\n# 5. Validate review scores\norder_reviews = order_reviews[order_reviews['review_score'].between(1, 5)]\n\n# 6. Creative addition: Calculate response time\norder_reviews['response_time_days'] = (order_reviews['review_answer_timestamp'] - \n                                       order_reviews['review_creation_date']).dt.days\n\n# 7. Save the cleaned file\norder_reviews.to_csv(output_path + 'cleaned_olist_order_reviews_dataset.csv', index=False)\nprint(\"Saved cleaned order reviews dataset as 'cleaned_olist_order_reviews_dataset.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:53:53.587874Z","iopub.execute_input":"2025-02-28T14:53:53.588229Z","iopub.status.idle":"2025-02-28T14:53:54.567738Z","shell.execute_reply.started":"2025-02-28T14:53:53.588200Z","shell.execute_reply":"2025-02-28T14:53:54.566413Z"}},"outputs":[{"name":"stdout","text":"Saved cleaned order reviews dataset as 'cleaned_olist_order_reviews_dataset.csv'\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"---\n---\n## 4. olist_order_items_dataset","metadata":{}},{"cell_type":"code","source":"# 1. Inspect the data\n\nprint(\"Initial Order Items Dataset Info:\")\nprint(order_items.info())\nprint(\"\\nMissing Values:\")\nprint(order_items.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:55:17.657409Z","iopub.execute_input":"2025-02-28T14:55:17.657800Z","iopub.status.idle":"2025-02-28T14:55:17.723388Z","shell.execute_reply.started":"2025-02-28T14:55:17.657772Z","shell.execute_reply":"2025-02-28T14:55:17.722274Z"}},"outputs":[{"name":"stdout","text":"Initial Order Items Dataset Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 112650 entries, 0 to 112649\nData columns (total 7 columns):\n #   Column               Non-Null Count   Dtype  \n---  ------               --------------   -----  \n 0   order_id             112650 non-null  object \n 1   order_item_id        112650 non-null  int64  \n 2   product_id           112650 non-null  object \n 3   seller_id            112650 non-null  object \n 4   shipping_limit_date  112650 non-null  object \n 5   price                112650 non-null  float64\n 6   freight_value        112650 non-null  float64\ndtypes: float64(2), int64(1), object(4)\nmemory usage: 6.0+ MB\nNone\n\nMissing Values:\norder_id               0\norder_item_id          0\nproduct_id             0\nseller_id              0\nshipping_limit_date    0\nprice                  0\nfreight_value          0\ndtype: int64\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# 2. Drop rows with missing critical IDs\norder_items = order_items.dropna(subset=['order_id', 'product_id', 'seller_id'])\n\n# 3. Fill missing shipping_limit_date\norder_items['shipping_limit_date'] = order_items['shipping_limit_date'].fillna('2099-12-31')\n\n# 4. Convert data types\norder_items['order_id'] = order_items['order_id'].astype(str)\norder_items['product_id'] = order_items['product_id'].astype(str)\norder_items['seller_id'] = order_items['seller_id'].astype(str)\norder_items['shipping_limit_date'] = pd.to_datetime(order_items['shipping_limit_date'], errors='coerce')\norder_items['price'] = order_items['price'].astype(float)\norder_items['freight_value'] = order_items['freight_value'].astype(float)\n\n# 5. Ensure non-negative values\norder_items['price'] = order_items['price'].clip(lower=0)\norder_items['freight_value'] = order_items['freight_value'].clip(lower=0)\n\n# 6. Creative addition: Calculate total cost\norder_items['total_cost'] = order_items['price'] + order_items['freight_value']\n\n# 7. Save the cleaned file\norder_items.to_csv(output_path + 'cleaned_olist_order_items_dataset.csv', index=False)\nprint(\"Saved cleaned order items dataset as 'cleaned_olist_order_items_dataset.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:58:06.437772Z","iopub.execute_input":"2025-02-28T14:58:06.438173Z","iopub.status.idle":"2025-02-28T14:58:07.456259Z","shell.execute_reply.started":"2025-02-28T14:58:06.438147Z","shell.execute_reply":"2025-02-28T14:58:07.455070Z"}},"outputs":[{"name":"stdout","text":"Saved cleaned order items dataset as 'cleaned_olist_order_items_dataset.csv'\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"---\n---\n## 5. olist_products_dataset","metadata":{}},{"cell_type":"code","source":"# 1. Inspect the data\n\nprint(\"Initial Products Dataset Info:\")\nprint(products.info())\nprint(\"\\nMissing Values:\")\nprint(products.isnull().sum())\nprint(\"\\nDuplicate Product IDs:\")\nprint(products['product_id'].duplicated().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:58:50.054931Z","iopub.execute_input":"2025-02-28T14:58:50.055335Z","iopub.status.idle":"2025-02-28T14:58:50.082281Z","shell.execute_reply.started":"2025-02-28T14:58:50.055309Z","shell.execute_reply":"2025-02-28T14:58:50.081231Z"}},"outputs":[{"name":"stdout","text":"Initial Products Dataset Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 32951 entries, 0 to 32950\nData columns (total 9 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   product_id                  32951 non-null  object \n 1   product_category_name       32341 non-null  object \n 2   product_name_lenght         32341 non-null  float64\n 3   product_description_lenght  32341 non-null  float64\n 4   product_photos_qty          32341 non-null  float64\n 5   product_weight_g            32949 non-null  float64\n 6   product_length_cm           32949 non-null  float64\n 7   product_height_cm           32949 non-null  float64\n 8   product_width_cm            32949 non-null  float64\ndtypes: float64(7), object(2)\nmemory usage: 2.3+ MB\nNone\n\nMissing Values:\nproduct_id                      0\nproduct_category_name         610\nproduct_name_lenght           610\nproduct_description_lenght    610\nproduct_photos_qty            610\nproduct_weight_g                2\nproduct_length_cm               2\nproduct_height_cm               2\nproduct_width_cm                2\ndtype: int64\n\nDuplicate Product IDs:\n0\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# 2. Handle missing values\n# Drop rows with missing product_id\nproducts = products.dropna(subset=['product_id'])\nprint(\"Dropped rows with missing product_id\")\n\n# Fill missing product_category_name with 'Unknown'\nproducts['product_category_name'] = products['product_category_name'].fillna('Unknown')\n\n# Numerical columns: impute with median to maintain distribution\nnumeric_cols = ['product_name_lenght', 'product_description_lenght', 'product_photos_qty',\n                'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']\nfor col in numeric_cols:\n    products[col] = products[col].fillna(products[col].median())\n    print(f\"Imputed missing values in {col} with median\")\n    \n# 3. Ensure data type consistency\nproducts['product_id'] = products['product_id'].astype(str)\nproducts['product_category_name'] = products['product_category_name'].astype(str)\nfor col in numeric_cols:\n    products[col] = products[col].astype(float)  # Use float to accommodate potential decimals\n\n# 4. Validate numerical data\n# Check for negative or unrealistic values\nfor col in numeric_cols:\n    if (products[col] < 0).sum() > 0:\n        products.loc[products[col] < 0, col] = products[col].median()\n        print(f\"Replaced negative values in {col} with median\")\n\n    # Cap extreme outliers (e.g., > 99th percentile)\n    upper_limit = products[col].quantile(0.99)\n    products[col] = products[col].clip(lower=0, upper=upper_limit)  # Fixed syntax\n    print(f\"Capped {col} at 99th percentile: {upper_limit}\")\n\n# 5. Check for duplicates\nproducts = products.drop_duplicates(subset=['product_id'], keep='first')\nprint(f\"Removed {products['product_id'].duplicated().sum()} duplicate product IDs\")\n\n# 6. Creative Addition - Calculate product volume\nproducts['product_volume_cm3'] = (products['product_length_cm'] * \n                                  products['product_height_cm'] * \n                                  products['product_width_cm'])\nprint(\"Added product_volume_cm3 for size-related analysis\")\n\n# 7. Creative Addition - Flag heavy products\nproducts['is_heavy'] = (products['product_weight_g'] > products['product_weight_g'].quantile(0.75)).astype(int)\nprint(\"Added is_heavy flag for logistics insights\")\n\n# 8. Save the cleaned dataset\nproducts.to_csv(output_path + 'cleaned_products_dataset.csv', index=False)\nprint(\"Products dataset cleaned and saved as 'cleaned_products_dataset.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T15:01:55.024777Z","iopub.execute_input":"2025-02-28T15:01:55.025395Z","iopub.status.idle":"2025-02-28T15:01:55.353301Z","shell.execute_reply.started":"2025-02-28T15:01:55.025347Z","shell.execute_reply":"2025-02-28T15:01:55.352134Z"}},"outputs":[{"name":"stdout","text":"Dropped rows with missing product_id\nImputed missing values in product_name_lenght with median\nImputed missing values in product_description_lenght with median\nImputed missing values in product_photos_qty with median\nImputed missing values in product_weight_g with median\nImputed missing values in product_length_cm with median\nImputed missing values in product_height_cm with median\nImputed missing values in product_width_cm with median\nCapped product_name_lenght at 99th percentile: 63.0\nCapped product_description_lenght at 99th percentile: 3274.5\nCapped product_photos_qty at 99th percentile: 8.0\nCapped product_weight_g at 99th percentile: 22537.5\nCapped product_length_cm at 99th percentile: 100.0\nCapped product_height_cm at 99th percentile: 69.0\nCapped product_width_cm at 99th percentile: 63.0\nRemoved 0 duplicate product IDs\nAdded product_volume_cm3 for size-related analysis\nAdded is_heavy flag for logistics insights\nProducts dataset cleaned and saved as 'cleaned_products_dataset.csv'\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"---\n---\n## 6. olist_geolocation_dataset","metadata":{}},{"cell_type":"code","source":"# 1. Inspect the data\n\nprint(\"Initial Geolocation Dataset Info:\")\nprint(geolocation.info())\nprint(\"\\nMissing Values:\")\nprint(geolocation.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T15:03:07.798287Z","iopub.execute_input":"2025-02-28T15:03:07.798717Z","iopub.status.idle":"2025-02-28T15:03:08.028383Z","shell.execute_reply.started":"2025-02-28T15:03:07.798685Z","shell.execute_reply":"2025-02-28T15:03:08.027101Z"}},"outputs":[{"name":"stdout","text":"Initial Geolocation Dataset Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000163 entries, 0 to 1000162\nData columns (total 5 columns):\n #   Column                       Non-Null Count    Dtype  \n---  ------                       --------------    -----  \n 0   geolocation_zip_code_prefix  1000163 non-null  int64  \n 1   geolocation_lat              1000163 non-null  float64\n 2   geolocation_lng              1000163 non-null  float64\n 3   geolocation_city             1000163 non-null  object \n 4   geolocation_state            1000163 non-null  object \ndtypes: float64(2), int64(1), object(2)\nmemory usage: 38.2+ MB\nNone\n\nMissing Values:\ngeolocation_zip_code_prefix    0\ngeolocation_lat                0\ngeolocation_lng                0\ngeolocation_city               0\ngeolocation_state              0\ndtype: int64\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# 2. Fill missing city and state\ngeolocation['geolocation_city'] = geolocation['geolocation_city'].fillna('Unknown')\ngeolocation['geolocation_state'] = geolocation['geolocation_state'].fillna('Unknown')\n\n# 3. Convert data types\ngeolocation['geolocation_zip_code_prefix'] = geolocation['geolocation_zip_code_prefix'].astype(str)\ngeolocation['geolocation_lat'] = geolocation['geolocation_lat'].astype(float)\ngeolocation['geolocation_lng'] = geolocation['geolocation_lng'].astype(float)\n\n# 4. Standardize text\ngeolocation['geolocation_city'] = geolocation['geolocation_city'].str.strip().str.title()\ngeolocation['geolocation_state'] = geolocation['geolocation_state'].str.strip().str.upper()\n\n# 5. Remove duplicates\ngeolocation = geolocation.drop_duplicates(subset=['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng'], keep='first')\n\n# 6. Creative addition: Validate lat/lng for Brazil (lat: -33.75 to 5.27, lng: -73.99 to -32.39)\ngeolocation['is_valid_coords'] = (geolocation['geolocation_lat'].between(-33.75, 5.27) & \n                                  geolocation['geolocation_lng'].between(-73.99, -32.39)).astype(int)\n\n# 7. Save the cleaned file\ngeolocation.to_csv(output_path + 'cleaned_olist_geolocation_dataset.csv', index=False)\nprint(\"Saved cleaned geolocation dataset as 'cleaned_olist_geolocation_dataset.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T15:05:19.678985Z","iopub.execute_input":"2025-02-28T15:05:19.679385Z","iopub.status.idle":"2025-02-28T15:05:25.066785Z","shell.execute_reply.started":"2025-02-28T15:05:19.679358Z","shell.execute_reply":"2025-02-28T15:05:25.065635Z"}},"outputs":[{"name":"stdout","text":"Saved cleaned geolocation dataset as 'cleaned_olist_geolocation_dataset.csv'\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"---\n---\n## 7. product_category_name_translation","metadata":{}},{"cell_type":"code","source":"# 1. Inspect the data\n\nprint(\"Initial Categories Dataset Info:\")\nprint(categories.info())\nprint(\"\\nMissing Values:\")\nprint(categories.isnull().sum())\nprint(\"\\nDuplicate Categories:\")\nprint(categories['product_category_name'].duplicated().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T15:06:25.974724Z","iopub.execute_input":"2025-02-28T15:06:25.975087Z","iopub.status.idle":"2025-02-28T15:06:25.989481Z","shell.execute_reply.started":"2025-02-28T15:06:25.975062Z","shell.execute_reply":"2025-02-28T15:06:25.988301Z"}},"outputs":[{"name":"stdout","text":"Initial Categories Dataset Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 71 entries, 0 to 70\nData columns (total 2 columns):\n #   Column                         Non-Null Count  Dtype \n---  ------                         --------------  ----- \n 0   product_category_name          71 non-null     object\n 1   product_category_name_english  71 non-null     object\ndtypes: object(2)\nmemory usage: 1.2+ KB\nNone\n\nMissing Values:\nproduct_category_name            0\nproduct_category_name_english    0\ndtype: int64\n\nDuplicate Categories:\n0\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# 2. Handle missing values\n# Drop rows with missing category names (either Portuguese or English)\ncategories = categories.dropna(subset=['product_category_name', 'product_category_name_english'])\nprint(f\"Dropped {categories.isnull().sum().sum()} rows with missing category names\")\n\n# 3. Ensure data type consistency\ncategories['product_category_name'] = categories['product_category_name'].astype(str)\ncategories['product_category_name_english'] = categories['product_category_name_english'].astype(str)\n\n# 4. Standardize text data\n# Remove whitespace and standardize formatting\ncategories['product_category_name'] = categories['product_category_name'].str.strip().str.lower()\ncategories['product_category_name_english'] = categories['product_category_name_english'].str.strip().str.lower().str.replace(' ', '_')\n\n# 5. Check for duplicates\n# Remove duplicate Portuguese category names, keeping the first translation\ncategories = categories.drop_duplicates(subset=['product_category_name'], keep='first')\nprint(f\"Removed {categories['product_category_name'].duplicated().sum()} duplicate category names\")\n\n# 6. Creative Addition - Add category grouping\n# Group categories into broader supercategories for advanced analysis\nsupercategory_mapping = {\n    'health_beauty': 'Personal Care', 'perfumery': 'Personal Care', 'baby': 'Personal Care',\n    'computers_accessories': 'Electronics', 'telephony': 'Electronics', 'fixed_telephony': 'Electronics',\n    'electronics': 'Electronics', 'home_appliances': 'Electronics', 'home_appliances_2': 'Electronics',\n    'auto': 'Automotive', 'construction_tools_construction': 'Home & Garden', 'garden_tools': 'Home & Garden',\n    'bed_bath_table': 'Home & Garden', 'furniture_decor': 'Home & Garden', 'housewares': 'Home & Garden',\n    'sports_leisure': 'Sports & Leisure', 'fashion_bags_accessories': 'Fashion', 'fashion_shoes': 'Fashion',\n    'fashion_male_clothing': 'Fashion', 'fashion_underwear_beach': 'Fashion', 'fashion_sport': 'Fashion',\n    'fashion_female_clothing': 'Fashion', 'fashion_childrens_clothes': 'Fashion',\n    'toys': 'Entertainment', 'consoles_games': 'Entertainment', 'musical_instruments': 'Entertainment',\n    'food_drink': 'Food & Beverage', 'food': 'Food & Beverage', 'drinks': 'Food & Beverage',\n    # Add more mappings as needed\n}\ncategories['supercategory'] = categories['product_category_name_english'].map(supercategory_mapping).fillna('Other')\nprint(\"Added supercategory column for broader category analysis\")\n\n# 7. Save the cleaned dataset\ncategories.to_csv(output_path + 'cleaned_product_category_name_translation.csv', index=False)\nprint(\"Categories dataset cleaned and saved as 'cleaned_product_category_name_translation.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T15:08:55.446151Z","iopub.execute_input":"2025-02-28T15:08:55.446475Z","iopub.status.idle":"2025-02-28T15:08:55.466270Z","shell.execute_reply.started":"2025-02-28T15:08:55.446451Z","shell.execute_reply":"2025-02-28T15:08:55.465194Z"}},"outputs":[{"name":"stdout","text":"Dropped 0 rows with missing category names\nRemoved 0 duplicate category names\nAdded supercategory column for broader category analysis\nCategories dataset cleaned and saved as 'cleaned_product_category_name_translation.csv'\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"---\n---\n## 8. olist_orders_dataset","metadata":{}},{"cell_type":"code","source":"# 1. Inspect the data\n\nprint(\"Initial Orders Dataset Info:\")\nprint(orders.info())\nprint(\"\\nMissing Values:\")\nprint(orders.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T15:10:45.838247Z","iopub.execute_input":"2025-02-28T15:10:45.838633Z","iopub.status.idle":"2025-02-28T15:10:45.934918Z","shell.execute_reply.started":"2025-02-28T15:10:45.838605Z","shell.execute_reply":"2025-02-28T15:10:45.933728Z"}},"outputs":[{"name":"stdout","text":"Initial Orders Dataset Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 99441 entries, 0 to 99440\nData columns (total 8 columns):\n #   Column                         Non-Null Count  Dtype \n---  ------                         --------------  ----- \n 0   order_id                       99441 non-null  object\n 1   customer_id                    99441 non-null  object\n 2   order_status                   99441 non-null  object\n 3   order_purchase_timestamp       99441 non-null  object\n 4   order_approved_at              99281 non-null  object\n 5   order_delivered_carrier_date   97658 non-null  object\n 6   order_delivered_customer_date  96476 non-null  object\n 7   order_estimated_delivery_date  99441 non-null  object\ndtypes: object(8)\nmemory usage: 6.1+ MB\nNone\n\nMissing Values:\norder_id                            0\ncustomer_id                         0\norder_status                        0\norder_purchase_timestamp            0\norder_approved_at                 160\norder_delivered_carrier_date     1783\norder_delivered_customer_date    2965\norder_estimated_delivery_date       0\ndtype: int64\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# 2. Drop rows with missing order_id\norders = orders.dropna(subset=['order_id'])\n\n# 3. Fill missing timestamps\ntimestamp_cols = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date',\n                  'order_delivered_customer_date', 'order_estimated_delivery_date']\nfor col in timestamp_cols:\n    orders[col] = orders[col].fillna('2099-12-31')\n\n# 4. Convert data types\norders['order_id'] = orders['order_id'].astype(str)\norders['customer_id'] = orders['customer_id'].astype(str)\norders['order_status'] = orders['order_status'].astype(str)\nfor col in timestamp_cols:\n    orders[col] = pd.to_datetime(orders[col], errors='coerce')\n\n# 5. Creative addition: Calculate delivery time\norders['delivery_time_days'] = (orders['order_delivered_customer_date'] - \n                                orders['order_purchase_timestamp']).dt.days\n\n# 6. Save the cleaned file\norders.to_csv(output_path + 'cleaned_olist_orders_dataset.csv', index=False)\nprint(\"Saved cleaned orders dataset as 'cleaned_olist_orders_dataset.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T15:12:39.388726Z","iopub.execute_input":"2025-02-28T15:12:39.389119Z","iopub.status.idle":"2025-02-28T15:12:40.960123Z","shell.execute_reply.started":"2025-02-28T15:12:39.389094Z","shell.execute_reply":"2025-02-28T15:12:40.958849Z"}},"outputs":[{"name":"stdout","text":"Saved cleaned orders dataset as 'cleaned_olist_orders_dataset.csv'\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"---\n---\n## 9. olist_order_payments_dataset","metadata":{}},{"cell_type":"code","source":"# 1. Inspect the data\n\nprint(\"Initial Order Payments Dataset Info:\")\nprint(order_payments.info())\nprint(\"\\nMissing Values:\")\nprint(order_payments.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T15:13:04.446931Z","iopub.execute_input":"2025-02-28T15:13:04.447266Z","iopub.status.idle":"2025-02-28T15:13:04.484966Z","shell.execute_reply.started":"2025-02-28T15:13:04.447241Z","shell.execute_reply":"2025-02-28T15:13:04.483814Z"}},"outputs":[{"name":"stdout","text":"Initial Order Payments Dataset Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 103886 entries, 0 to 103885\nData columns (total 5 columns):\n #   Column                Non-Null Count   Dtype  \n---  ------                --------------   -----  \n 0   order_id              103886 non-null  object \n 1   payment_sequential    103886 non-null  int64  \n 2   payment_type          103886 non-null  object \n 3   payment_installments  103886 non-null  int64  \n 4   payment_value         103886 non-null  float64\ndtypes: float64(1), int64(2), object(2)\nmemory usage: 4.0+ MB\nNone\n\nMissing Values:\norder_id                0\npayment_sequential      0\npayment_type            0\npayment_installments    0\npayment_value           0\ndtype: int64\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# 2. Drop rows with missing order_id\norder_payments = order_payments.dropna(subset=['order_id'])\n\n# 3. Fill missing payment_type\norder_payments['payment_type'] = order_payments['payment_type'].fillna('Unknown')\n\n# 4. Convert data types\norder_payments['order_id'] = order_payments['order_id'].astype(str)\norder_payments['payment_type'] = order_payments['payment_type'].astype(str)\norder_payments['payment_value'] = order_payments['payment_value'].astype(float)\n\n# 5. Ensure non-negative payment values\norder_payments['payment_value'] = order_payments['payment_value'].clip(lower=0)\n\n# 6. Creative addition: Categorize payments\norder_payments['payment_category'] = order_payments['payment_type'].apply(\n    lambda x: 'Card' if 'card' in x.lower() else 'Other'\n)\n\n# 7. Save the cleaned file\norder_payments.to_csv(output_path + 'cleaned_olist_order_payments_dataset.csv', index=False)\nprint(\"Saved cleaned order payments dataset as 'cleaned_olist_order_payments_dataset.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T15:15:18.998779Z","iopub.execute_input":"2025-02-28T15:15:18.999179Z","iopub.status.idle":"2025-02-28T15:15:19.371040Z","shell.execute_reply.started":"2025-02-28T15:15:18.999152Z","shell.execute_reply":"2025-02-28T15:15:19.369968Z"}},"outputs":[{"name":"stdout","text":"Saved cleaned order payments dataset as 'cleaned_olist_order_payments_dataset.csv'\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"---\n---\n## 1. Merging_Datasets_Cleaned","metadata":{}},{"cell_type":"code","source":"# 1. Load all cleaned datasets\n\norders = pd.read_csv(output_path + 'cleaned_olist_orders_dataset.csv')\norder_items = pd.read_csv(output_path + 'cleaned_olist_order_items_dataset.csv')\nproducts = pd.read_csv(output_path + 'cleaned_products_dataset.csv')\ncategory_translation = pd.read_csv(output_path + 'cleaned_product_category_name_translation.csv')\nsellers = pd.read_csv(output_path + 'cleaned_sellers_dataset.csv')\ncustomers = pd.read_csv(output_path + 'cleaned_olist_customers_dataset.csv')\norder_payments = pd.read_csv(output_path + 'cleaned_olist_order_payments_dataset.csv')\norder_reviews = pd.read_csv(output_path + 'cleaned_olist_order_reviews_dataset.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T15:17:52.094806Z","iopub.execute_input":"2025-02-28T15:17:52.095208Z","iopub.status.idle":"2025-02-28T15:17:53.979393Z","shell.execute_reply.started":"2025-02-28T15:17:52.095182Z","shell.execute_reply":"2025-02-28T15:17:53.978001Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# 2. Merge step-by-step using left joins to preserve all orders\nmerged_data = orders.merge(order_items, on='order_id', how='left') \\\n                    .merge(products, on='product_id', how='left') \\\n                    .merge(category_translation, on='product_category_name', how='left') \\\n                    .merge(sellers, on='seller_id', how='left') \\\n                    .merge(customers, on='customer_id', how='left') \\\n                    .merge(order_payments, on='order_id', how='left') \\\n                    .merge(order_reviews, on='order_id', how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T15:18:43.909114Z","iopub.execute_input":"2025-02-28T15:18:43.909451Z","iopub.status.idle":"2025-02-28T15:18:45.284979Z","shell.execute_reply.started":"2025-02-28T15:18:43.909427Z","shell.execute_reply":"2025-02-28T15:18:45.283896Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# 3. Save the merged dataset\nmerged_data.to_csv(output_path + 'merged_olist_dataset.csv', index=False)\nprint(\"All cleaned datasets merged and saved as 'kaggle/working/merged_olist_dataset.csv'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T15:19:39.172623Z","iopub.execute_input":"2025-02-28T15:19:39.173077Z","iopub.status.idle":"2025-02-28T15:19:44.124808Z","shell.execute_reply.started":"2025-02-28T15:19:39.173046Z","shell.execute_reply":"2025-02-28T15:19:44.123499Z"}},"outputs":[{"name":"stdout","text":"All cleaned datasets merged and saved as 'kaggle/working/merged_olist_dataset.csv'\n","output_type":"stream"}],"execution_count":41}]}
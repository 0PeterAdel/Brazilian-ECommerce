{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../DataSet/\"\n",
    "\n",
    "sellers = pd.read_csv(root_path + 'olist_sellers_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Inspect the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Sellers Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3095 entries, 0 to 3094\n",
      "Data columns (total 4 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   seller_id               3095 non-null   object\n",
      " 1   seller_zip_code_prefix  3095 non-null   int64 \n",
      " 2   seller_city             3095 non-null   object\n",
      " 3   seller_state            3095 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 96.8+ KB\n",
      "None\n",
      "\n",
      "Missing Values:\n",
      "seller_id                 0\n",
      "seller_zip_code_prefix    0\n",
      "seller_city               0\n",
      "seller_state              0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Seller IDs:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial Sellers Dataset Info:\")\n",
    "print(sellers.info())\n",
    "print(\"\\nMissing Values:\")\n",
    "print(sellers.isnull().sum())\n",
    "print(\"\\nDuplicate Seller IDs:\")\n",
    "print(sellers['seller_id'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Handle missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in critical columns\n",
    "if sellers['seller_id'].isnull().sum() > 0:\n",
    "    sellers = sellers.dropna(subset=['seller_id'])  # Drop rows with missing seller_id\n",
    "    print(\"Dropped rows with missing seller_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For zip code, city, and state, fill missing values with 'Unknown' to preserve data\n",
    "sellers['seller_zip_code_prefix'] = sellers['seller_zip_code_prefix'].fillna('Unknown')\n",
    "sellers['seller_city'] = sellers['seller_city'].fillna('Unknown')\n",
    "sellers['seller_state'] = sellers['seller_state'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Ensure data type consistency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seller_id should be string\n",
    "sellers['seller_id'] = sellers['seller_id'].astype(str)\n",
    "# seller_zip_code_prefix might be numeric or string; convert to string for consistency\n",
    "sellers['seller_zip_code_prefix'] = sellers['seller_zip_code_prefix'].astype(str)\n",
    "# seller_city and seller_state should be strings\n",
    "sellers['seller_city'] = sellers['seller_city'].astype(str)\n",
    "sellers['seller_state'] = sellers['seller_state'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Standardize text data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading/trailing whitespace and convert cities/states to title case\n",
    "sellers['seller_city'] = sellers['seller_city'].str.strip().str.title()\n",
    "sellers['seller_state'] = sellers['seller_state'].str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Check for duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 duplicate seller IDs\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate seller_id entries, keeping the first occurrence\n",
    "sellers = sellers.drop_duplicates(subset=['seller_id'], keep='first')\n",
    "print(f\"Removed {sellers['seller_id'].duplicated().sum()} duplicate seller IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Validate state codes (Creative Touch)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brazilian states have 2-letter codes; flag any anomalies\n",
    "valid_states = {'AC', 'AL', 'AP', 'AM', 'BA', 'CE', 'DF', 'ES', 'GO', 'MA', 'MT', \n",
    "                'MS', 'MG', 'PA', 'PB', 'PR', 'PE', 'PI', 'RJ', 'RN', 'RS', 'RO', \n",
    "                'RR', 'SC', 'SP', 'SE', 'TO'}\n",
    "sellers['is_valid_state'] = sellers['seller_state'].isin(valid_states).astype(int)\n",
    "invalid_states = sellers[~sellers['seller_state'].isin(valid_states)]['seller_state'].unique()\n",
    "if len(invalid_states) > 0:\n",
    "    print(f\"Invalid state codes detected: {invalid_states}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7: Creative Addition - Derive region from state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Brazilian states to regions for geographic analysis\n",
    "state_to_region = {\n",
    "    'AC': 'North', 'AM': 'North', 'AP': 'North', 'PA': 'North', 'RO': 'North', 'RR': 'North', 'TO': 'North',\n",
    "    'AL': 'Northeast', 'BA': 'Northeast', 'CE': 'Northeast', 'MA': 'Northeast', 'PB': 'Northeast', \n",
    "    'PE': 'Northeast', 'PI': 'Northeast', 'RN': 'Northeast', 'SE': 'Northeast',\n",
    "    'DF': 'Central-West', 'GO': 'Central-West', 'MT': 'Central-West', 'MS': 'Central-West',\n",
    "    'ES': 'Southeast', 'MG': 'Southeast', 'RJ': 'Southeast', 'SP': 'Southeast',\n",
    "    'PR': 'South', 'RS': 'South', 'SC': 'South'\n",
    "}\n",
    "sellers['seller_region'] = sellers['seller_state'].map(state_to_region).fillna('Unknown')\n",
    "print(\"Added seller_region column for geographic insights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8: Save the cleaned dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sellers dataset cleaned and saved as 'cleaned_sellers_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "sellers.to_csv('./Data_Cleaned/cleaned_sellers_dataset.csv', index=False)\n",
    "print(\"Sellers dataset cleaned and saved as './Data_Cleaned/cleaned_sellers_dataset.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
